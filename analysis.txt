N= 1000   TSTEPS = 20

                      average time            SPEEDUP

normale                 0.148 s                 1x

cpu_v1                  0.302 s                 0.49x
cpu_v2                  0.303 s                 0.49x
cpu_v3                  0.150 s                 0.99x
cpu_v4                  0.303 s                 0.49x

gpu_v1                  23.8  s                 0.006x
gpu_v2                  1.8   s                 0.08x
gpu_v3                  1.65  s                 0.09x
gpu_v4                  0.6   s                 0.25x

----------------------------------------------------------------------------------------------
N= 4000   TSTEPS = 20

                      average time            SPEEDUP

normale                 2.22 s                  1x

cpu_v3                  2.22 s                  1x

gpu_v4                  2.15 s                  1.03x

-----------------------------------------------------------------------------------------------

N= 20   TSTEPS = 4000

                      average time            SPEEDUP

normale                 0.011 s                 1x

cpu_v3                  0.022 s                 0.5x

gpu_v4                  1.2   s                 0.01x

-----------------------------------------------------------------------------------------------

N= 4000   TSTEPS = 100

                      average time            SPEEDUP

normale                 11.22 s                 1x

cpu_v3                  11.22 s                 1x

gpu_v4                  6.9 s                   1.62x

-----------------------------------------------------------------------------------------------

.... spiega i risultati fino a qui,
        1.  i tsteps non vengono parallelizzati
        2. con il diminuire di N il numero di cicli parallelizzati cala drasticamente(tsteps*n^2) e l'utilità delle gpu è limitata
        3. con l'aumentare di N fino a 4000 la gpu comincia a essere più veloce della cpu
        4. l'aumentare di tsteps fa calare drasticamente le performance dei programmi gpu da v1 a v3 questo perchè per ogni step devono
                    eseguire una copia delle matrici, con v4 questo viene risolto
        5. prendendo i 3 programmi che si sono comportati meglio nelle varie categorie(normale, cpu_v3, gpu_v4) i tsteps non influiranno sul cambiare
                direattamente quale è più vantaggioso attualmente, questo perchè i cicli di tsteps non sono parallelizzati, tuttavia
                sia la preparazione dei thread cpu che gpu con copia dei dati in memoria e altri fattori necessitano un costo iniziale
                che nell'esecuzione del programma normale non c'è, di conseguenza in generale ci renderemo conto aumentando i tsteps che
                i programmi parallelizzati tendono a guadagnare sull'esecuzione normale, aumentando lo speedup





---------------------------------------------------------------------------------------
N= 20   TSTEPS = 4000

                      average time            SPEEDUP

normale                 0.011 s                 1x

cpu_v3                  0.022 s                 0.5x

gpu_v4                  1.2   s                 0.01x

cuda_v1                 0.161 s                 0.07x


-----------------------------------------------------------------------------------------------

N= 1024   TSTEPS = 100

                      average time                          SPEEDUP

normale                 0.855 s                              1x

cpu_v3                  0.860 s                              0.99x

gpu_v4                  1.5                                  0.57x

cuda_v1                 0.550 s (kernel + DtoD)              1.55x
                        0.044 s (memcpy)

-----------------------------------------------------------------------------------------------

N= 4000   TSTEPS = 100

                      average time                    SPEEDUP

normale                 11.22 s                         1x

cpu_v3                  11.22 s                         1x

gpu_v4                  6.9 s                           1.62x

cuda_v1                 4.561 s (kernel + DtoD)         2.46x
                        0.258 s (memcpy)


in generale per matrici di dimensioni maggiori l'alto numero di thread e blocchi della gpu risulta molto vantaggioso rispetto a cpu
o gpu gestita da OpenMP, si possono ancora fare molte ottimizzazioni al codice cuda, di seguito verranno comparate varie versioni
di kernel, la dimensione della matrice verrà ridotta per le versioni che puntano a memorizzare dati in shared per limitazioni di spazio.

shred mem per blocco: 48KB
shared mem per SM: 64KB

Byte occupati in base al numero di thread all'interno della shared mem: 8*(NUM_THREAD+2)^2 per blocco

NUM_THREAD=8 -> 8*10*10=800 byte x blocco -> 800*32 = 25600 Byte all'interno della shared nel SM (numero massimo di thread è 2048 su SM)
NUM_THREAD=16 -> 8*18*18=2592 byte x blocco -> 2592*8 = 20736 Byte all'interno della shared nel SM (numero massimo di thread è 2048 su SM)
NUM_THREAD=32 -> 8*34*34=9248 byte x blocco -> 9248*2 = 18496 Byte all'interno della shared nel SM (numero massimo di thread è 2048 su SM)

-----------------------------------------------------------------------------------------------

N= 1024   TSTEPS = 1000

                      average time            SPEEDUP

normale                  8.45 s                 1x

cpu_v3                   8.44 s                 1x

gpu_v4                   6.86 s                 1.23x

cuda_v1                  2.9  s                 2,91x
                         +45 ms  (memcpy H-D)

cuda_v2                  2.7  s                 3.12x
                         +7  ms  (memcpy HtoH)

cuda_v3                  2.5  s                 3.38x

cuda_v4                  2.6  s                 3.25x

-----------------------------------------------------------------------------------------------

N= 4096   TSTEPS = 2000

                      average time            SPEEDUP

normale                  243 s                   1x

cuda_v2                  78.2  s                 3.11x
                         +7  ms  (memcpy HtoH)

cuda_v3                  77.9  s                 3.12x


cuda_v4                  78.3  s                 3.11x






